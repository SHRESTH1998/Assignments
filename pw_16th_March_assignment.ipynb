{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ad9fa272-63e2-4192-972c-b07b4bb51a9b",
      "metadata": {
        "id": "ad9fa272-63e2-4192-972c-b07b4bb51a9b"
      },
      "source": [
        "Q1. Ans"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08b6688b-6a17-4212-a69e-f7dde1bf2fe5",
      "metadata": {
        "id": "08b6688b-6a17-4212-a69e-f7dde1bf2fe5"
      },
      "source": [
        "For the uninitiated, in data science, overfitting simply means that the learning model is far too dependent on training data while underfitting means that the model has a poor relationship with the training data. Ideally, both of these should not exist in models, but they usually are hard to eliminate.\n",
        "\n",
        "Consequences of Overfitting\n",
        "When the model memorizes the noise and fits too closely to the training set, the model becomes “overfitted,” and it is unable to generalize well to new data. If a model cannot generalize well to new data, then it will not be able to perform the classification or prediction tasks that it was intended for.\n",
        "\n",
        "Consequences of Underfitting\n",
        "Like overfitting, when a model is underfitted, it cannot establish the dominant trend within the data, resulting in training errors and poor performance of the model. If a model cannot generalize well to new data, then it cannot be leveraged for classification or prediction tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00c83b81-9159-4440-9c30-8c3ed4b1e42d",
      "metadata": {
        "id": "00c83b81-9159-4440-9c30-8c3ed4b1e42d"
      },
      "source": [
        "Q2. Ans"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90e277f5-2be7-4a72-b682-502f7ca2c488",
      "metadata": {
        "id": "90e277f5-2be7-4a72-b682-502f7ca2c488"
      },
      "source": [
        "1. Train with more data\n",
        "With the increase in the training data, the crucial features to be extracted become prominent. The model can recognize the relationship between the input attributes and the output variable. The only assumption in this method is that the data to be fed into the model should be clean; otherwise, it would worsen the problem of overfitting.\n",
        "\n",
        "2. Data augmentation\n",
        "An alternative method to training with more data is data augmentation, which is less expensive and safer than the previous method. Data augmentation makes a sample data look slightly different every time the model processes it.\n",
        "\n",
        "3. Addition of noise to the input data\n",
        "Another similar option as data augmentation is adding noise to the input and output data. Adding noise to the input makes the model stable without affecting data quality and privacy while adding noise to the output makes the data more diverse. Noise addition should be done in limit so that it does not make the data incorrect or too different.\n",
        "\n",
        "4. Feature selection\n",
        "Every model has several parameters or features depending upon the number of layers, number of neurons, etc.  The model can detect many redundant features or features determinable from other features leading to unnecessary complexity. We very well know that the more complex the model, the higher the chances of the model to overfit.\n",
        "\n",
        "5. Cross-validation\n",
        "Cross-validation is a robust measure to prevent overfitting. The complete dataset is split into parts. In standard K-fold cross-validation, we need to partition the data into k folds. Then, we iteratively train the algorithm on k-1 folds while using the remaining holdout fold as the test set. This method allows us to tune the hyperparameters of the neural network or machine learning model and test it using completely unseen data.\n",
        "\n",
        "6. Simplify data\n",
        "Till now, we have come across model complexity to be one of the top reasons for overfitting. The data simplification method is used to reduce overfitting by decreasing the complexity of the model to make it simple enough that it does not overfit. Some of the procedures include pruning a decision tree, reducing the number of parameters in a neural network, and using dropout on a neutral network.\n",
        "\n",
        "7. Regularization\n",
        "If overfitting occurs when a model is too complex, reducing the number of features makes sense. Regularization methods like Lasso, L1 can be beneficial if we do not know which features to remove from our model. Regularization applies a \"penalty\" to the input parameters with the larger coefficients, which subsequently limits the model's variance.\n",
        "\n",
        "8. Ensembling\n",
        "It is a machine learning technique that combines several base models to produce one optimal predictive model. In Ensemble learning,  the predictions are aggregated to identify the most popular result. Well-known ensemble methods include bagging and boosting, which prevents overfitting as an ensemble model is made from the aggregation of multiple models.\n",
        "\n",
        "9. Early stopping\n",
        "This method aims to pause the model's training before memorizing noise and random fluctuations from the data. There can be a risk that the model stops training too soon, leading to underfitting. One has to come to an optimum time/iterations the model should train.\n",
        "\n",
        "10. Adding dropout layers\n",
        "Large weights in a neural network signify a more complex network. Probabilistically dropping out nodes in the network is a simple and effective method to prevent overfitting. In regularization, some number of layer outputs are randomly ignored or “dropped out” to reduce the complexity of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c0fe2de-e25f-4312-b1e3-3f7fd4d46ece",
      "metadata": {
        "id": "0c0fe2de-e25f-4312-b1e3-3f7fd4d46ece"
      },
      "source": [
        "Q3. Ans"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d8a26c9-2b4b-48d9-8f44-8a557389e2dd",
      "metadata": {
        "id": "6d8a26c9-2b4b-48d9-8f44-8a557389e2dd"
      },
      "source": [
        "Underfitting is a scenario in data science where a data model is unable to capture the relationship between the input and output variables accurately, generating a high error rate on both the training set and unseen data.\n",
        "\n",
        "Underfitting of machine learning models happens when you are not able to reduce the training error. This can happen in some of the following scenarios: When the training set has far fewer observations than variables, this may lead to underfitting or low bias machine learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2159716e-bb29-42b5-9dff-314dd6a618b3",
      "metadata": {
        "id": "2159716e-bb29-42b5-9dff-314dd6a618b3"
      },
      "source": [
        "Q4. Ans"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5103f721-a47a-44ce-a4aa-b8cc045d7f44",
      "metadata": {
        "id": "5103f721-a47a-44ce-a4aa-b8cc045d7f44"
      },
      "source": [
        "In statistics and machine learning, the bias–variance tradeoff is the property of a model that the variance of the parameter estimated across samples can be reduced by increasing the bias in the estimated parameters.\n",
        "\n",
        "Bias and variance are inversely connected. It is impossible to have an ML model with a low bias and a low variance. When a data engineer modifies the ML algorithm to better fit a given data set, it will lead to low bias—but it will increase variance.\n",
        "\n",
        "When we modify the ML algorithm to better fit a given data set, it will in turn lead to low bias but will increase the variance. This way, the model will fit with the data set while increasing the chances of inaccurate predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "669b0149-4d31-48f0-ae30-03da924d25d8",
      "metadata": {
        "id": "669b0149-4d31-48f0-ae30-03da924d25d8"
      },
      "source": [
        "Q5. Ans"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ffc0413-494b-40f5-b9af-7515ee6d9d63",
      "metadata": {
        "id": "0ffc0413-494b-40f5-b9af-7515ee6d9d63"
      },
      "source": [
        "1. Action that could (potentially) limit overfitting:\n",
        "\n",
        "a. We can use a Cross-validation (CV) scheme.\n",
        "b. Reduce the complexity of the model (make the model less complex).\n",
        "\n",
        "2. To (potentially) limit Underfitting\n",
        "In that case, there are 2 gold standard approaches:\n",
        "\n",
        "a. Try another model\n",
        "b. Increase the complexity of the current model\n",
        "\n",
        "The best method to detect overfit models is by testing the machine learning models on more data with with comprehensive representation of possible input data values and types. Typically, part of the training data is used as test data to check for overfitting. A high error rate in the testing data indicates overfitting.\n",
        "We can determine whether a predictive model is underfitting or overfitting the training data by looking at the prediction error on the training data and the evaluation data. Your model is underfitting the training data when the model performs poorly on the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb328e8c-e12e-45f6-9339-fe51fd028f82",
      "metadata": {
        "id": "fb328e8c-e12e-45f6-9339-fe51fd028f82"
      },
      "source": [
        "Q6. Ans"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a662c6d1-d952-444f-8d85-0ce525693c8a",
      "metadata": {
        "id": "a662c6d1-d952-444f-8d85-0ce525693c8a"
      },
      "source": [
        "Bias is the amount that a model’s prediction differs from the target value, compared to the training data. Bias error results from simplifying the assumptions used in a model so the target functions are easier to approximate. Bias can be introduced by model selection. Data scientists conduct resampling to repeat the model building process and derive the average of prediction values.\n",
        "\n",
        "Variance indicates how much the estimate of the target function will alter if different training data were usedExternal link:open_in_new. In other words, variance describes how much a random variable differs from its expected value. Variance is based on a single training set. Variance measures the inconsistency of different predictions using different training sets — it’s not a measure of overall accuracy.\n",
        "\n",
        "Examples of high-variance machine learning algorithms include: Decision Trees, k-Nearest Neighbors and Support Vector Machines.\n",
        "\n",
        "Examples of high-bias machine learning algorithms include: Linear Regression, Linear Discriminant Analysis and Logistic Regression.\n",
        "\n",
        "Variance comes from highly complex models with a large number of features. Models with high bias will have low variance. Models with high variance will have a low bias."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99e7f924-d81d-40e9-9c72-3dfc9ef57ea9",
      "metadata": {
        "id": "99e7f924-d81d-40e9-9c72-3dfc9ef57ea9"
      },
      "source": [
        "Q7. Ans"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db31a048-bc7e-469f-8ae6-35df0ea314c1",
      "metadata": {
        "id": "db31a048-bc7e-469f-8ae6-35df0ea314c1"
      },
      "source": [
        "Regularization is a technique that adds information to a model to prevent the occurrence of overfitting. It is a type of regression that minimizes the coefficient estimates to zero to reduce the capacity (size) of a model. In this context, the reduction of the capacity of a model involves the removal of extra weights.\n",
        "\n",
        "Weight regularization is a step that helps in preventing overfitting by reducing the complexity of the models. There are various ways of regularization like L1 and L2 regularization. These methods mainly work by penalizing the weights of any function and these smaller weights lead to simpler models.\n",
        "\n",
        "We can solve the problem of overfitting by: Increasing the training data by data augmentation. Feature selection by choosing the best features and remove the useless/unnecessary features. Early stopping the training of deep learning models where the number of epochs is set high."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}