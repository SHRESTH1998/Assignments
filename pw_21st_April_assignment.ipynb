{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70d933d2",
   "metadata": {},
   "source": [
    "# Q1. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586ec7b0",
   "metadata": {},
   "source": [
    "The main difference between the Euclidean distance metric and the Manhattan distance metric in KNN is the way they measure the distance between two points in a feature space.\n",
    "\n",
    "Euclidean Distance:\n",
    "\n",
    "Euclidean distance is calculated as the straight-line distance between two points in a feature space.\n",
    "Formula: sqrt(sum((x1 - x2)^2))\n",
    "Euclidean distance considers the magnitude and direction of the differences between feature values.\n",
    "It assumes that all features have equal importance and contribute equally to the distance calculation.\n",
    "It is more sensitive to differences in features with larger scales or variances.\n",
    "Manhattan Distance:\n",
    "\n",
    "Manhattan distance is calculated as the sum of absolute differences between the coordinates of two points in a feature space.\n",
    "Formula: sum(abs(x1 - x2))\n",
    "Manhattan distance only considers the magnitude of the differences between feature values.\n",
    "It measures the distance in terms of the number of steps needed to move from one point to another along the axes of the feature space.\n",
    "It treats all features equally and does not take into account the direction or orientation of the differences.\n",
    "The choice of distance metric can affect the performance of a KNN classifier or regressor in the following ways:\n",
    "\n",
    "Feature Scaling: Euclidean distance is more sensitive to differences in feature scales or variances. If the features have significantly different scales, it can dominate the distance calculation and lead to biased results. In such cases, feature scaling (e.g., normalization or standardization) is important to ensure that all features contribute equally. Manhattan distance, on the other hand, is not as affected by feature scales and can be more robust in situations where scaling is challenging.\n",
    "\n",
    "Feature Correlation: Euclidean distance considers the direction and magnitude of the feature differences, which means it takes into account the correlations between features. If there are highly correlated features, Euclidean distance can capture their relationships and potentially improve the performance of the KNN algorithm. Manhattan distance, being based on absolute differences, treats features independently and may not capture the correlations as effectively.\n",
    "\n",
    "Decision Boundary: The choice of distance metric can affect the shape and orientation of the decision boundary in KNN classification. Euclidean distance tends to create circular decision boundaries, while Manhattan distance tends to create square or diamond-shaped decision boundaries. Depending on the underlying data distribution and the nature of the problem, one distance metric may be more appropriate than the other in capturing the decision boundary accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca610ef0",
   "metadata": {},
   "source": [
    "# Q2. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e963c09",
   "metadata": {},
   "source": [
    "The choice of k will largely depend on the input data as data with more outliers or noise will likely perform better with higher values of k. Overall, it is recommended to have an odd number for k to avoid ties in classification, and cross-validation tactics can help you choose the optimal k for your dataset.\n",
    "\n",
    "Elbow Curve Method\n",
    "For each k, calculate the total within-cluster sum of squares (WSS). This elbow point can be used to determine K. Perform K-means clustering with all these different values of K. For each of the K values, we calculate average distances to the centroid across all data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc152506",
   "metadata": {},
   "source": [
    "# Q3.Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab3143d",
   "metadata": {},
   "source": [
    "The experimental results show that the performance of KNN classifier depends significantly on the distance used, and the results showed large gaps between the performances of different distances.\n",
    "\n",
    "The choice between Euclidean distance and Manhattan distance depends on the nature of the data and the problem at hand. Here are some scenarios where one distance metric might be preferred over the other:\n",
    "\n",
    "1. Data with Varying Scales: If the features in the dataset have significantly different scales, Euclidean distance may be biased towards features with larger values. In such cases, it might be beneficial to use Manhattan distance, which is less sensitive to differences in scale.\n",
    "\n",
    "2. Categorical or Ordinal Features: When dealing with data that contains categorical or ordinal features, Manhattan distance can be more appropriate. It treats each feature equally and measures the difference based on the number of steps required to move from one category or level to another.\n",
    "\n",
    "3. Noise or Outliers: If the dataset contains noisy or outlier data points, Euclidean distance can be more sensitive to these anomalies. In such cases, Manhattan distance might be more robust as it only considers the differences between feature values without considering the magnitude.\n",
    "\n",
    "4. Data Distribution: The choice of distance metric can also depend on the underlying data distribution. For example, if the data points tend to be aligned along the axes (i.e., when there is a linear relationship between the features), Manhattan distance might be more suitable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a168d7",
   "metadata": {},
   "source": [
    "# Q4. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f166610",
   "metadata": {},
   "source": [
    "Some common hyperparameters in KNN classifiers and regressors are:\n",
    "\n",
    "1. Number of neighbors (k): This hyperparameter determines the number of nearest neighbors considered for making predictions. Increasing the value of k can make the model more robust to noise but might result in smoother decision boundaries, potentially leading to a loss of accuracy for complex patterns. On the other hand, decreasing k can make the model more sensitive to noise but might capture more local variations in the data. The optimal value of k depends on the dataset and should be chosen through experimentation.\n",
    "\n",
    "2. Distance metric: The choice of distance metric, such as Euclidean or Manhattan, can affect the model's performance. Different distance metrics may be more suitable for different types of data or problem domains. Experimenting with different distance metrics can help identify the most appropriate one for the given dataset.\n",
    "\n",
    "3. Weighting scheme: KNN allows for assigning weights to the neighbors based on their distance from the query point. Common weighting schemes include uniform weighting, where all neighbors have equal influence, and distance-based weighting, where closer neighbors have higher influence. Weighting schemes can be particularly useful when dealing with imbalanced datasets or when certain neighbors are more relevant than others.\n",
    "\n",
    "To tune these hyperparameters and improve model performance, one can follow these steps:\n",
    "\n",
    "1. Split the dataset: Divide the dataset into training and validation sets. This allows for evaluating the model's performance on unseen data.\n",
    "\n",
    "2. Define a performance metric: Choose an appropriate metric to evaluate the model's performance, such as accuracy, precision, recall, mean squared error (MSE), or R-squared. The choice of metric depends on the specific problem being solved.\n",
    "\n",
    "3. Grid search or random search: Perform a grid search or random search over the hyperparameter space, trying different combinations of hyperparameter values. For example, vary the values of k, distance metric, and weighting scheme. Evaluate the model's performance using cross-validation on the training set for each combination of hyperparameters.\n",
    "\n",
    "4. Select the best hyperparameters: Identify the hyperparameter combination that yields the best performance according to the chosen metric. This can be done by comparing the performance across different hyperparameter combinations.\n",
    "\n",
    "5. Evaluate on the validation set: Finally, evaluate the performance of the model with the selected hyperparameters on the validation set. This provides an estimate of how well the model generalizes to unseen data.\n",
    "\n",
    "By iteratively adjusting the hyperparameters and evaluating the model's performance, one can find the optimal combination of hyperparameters that maximizes the model's performance on the given task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ced3d5",
   "metadata": {},
   "source": [
    "# Q5. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee1fe82",
   "metadata": {},
   "source": [
    "The size of the training set can have an impact on the performance of a KNN classifier or regressor in several ways:\n",
    "\n",
    "1. Generalization: With a smaller training set, the model may struggle to capture the underlying patterns and relationships in the data, leading to poor generalization. This can result in overfitting, where the model performs well on the training set but poorly on unseen data. Increasing the size of the training set can help improve generalization by providing more diverse examples for the model to learn from.\n",
    "\n",
    "2. Model complexity: The size of the training set can influence the optimal model complexity. A smaller training set may not provide enough information for a complex model, leading to overfitting. In contrast, a larger training set can support a more complex model that can capture intricate patterns in the data.\n",
    "\n",
    "To optimize the size of the training set, the following techniques can be considered:\n",
    "\n",
    "1. Cross-validation: Use techniques like k-fold cross-validation to evaluate model performance on different subsets of the training set. This allows for assessing how the model's performance varies with different training set sizes. By analyzing the model's performance across different fold sizes, one can identify the point where increasing the training set size provides diminishing returns.\n",
    "\n",
    "2. Learning curves: Plot learning curves to visualize how the model's performance changes with varying training set sizes. Learning curves show the training and validation performance as a function of the training set size. They can help identify if the model is suffering from high bias (underfitting) or high variance (overfitting) due to the training set size. Adjustments can be made accordingly, such as collecting more data if the model shows high variance.\n",
    "\n",
    "3. Data augmentation: In some cases, it may be possible to augment the existing training data by creating synthetic samples. For example, in image classification, techniques like rotation, translation, and flipping can be applied to generate additional training examples. Data augmentation can effectively increase the effective size of the training set without the need for additional data collection.\n",
    "\n",
    "4. Data sampling techniques: If the training set is extremely large and computationally expensive to process, techniques such as random sampling or stratified sampling can be used to create smaller representative subsets of the data. These subsets can then be used as the training set for model development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eebfdd1",
   "metadata": {},
   "source": [
    "# Q6. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf627db",
   "metadata": {},
   "source": [
    "Here are some of the disadvantages of using the k-nearest neighbors algorithm:\n",
    "1. Associated computation cost is high as it stores all the training data.\n",
    "2. Requires high memory storage.\n",
    "3. Need to determine the value of K.\n",
    "4. Prediction is slow if the value of N is high.\n",
    "5. Sensitive to irrelevant features.\n",
    "\n",
    "KNN is sensitive to outliers and missing values and hence we first need to impute the missing values and get rid of the outliers before applying the KNN algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8475e0cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
