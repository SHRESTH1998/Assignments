{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6dc9fbf",
   "metadata": {},
   "source": [
    "# Q1. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d13a62",
   "metadata": {},
   "source": [
    "An Activation Function decides whether a neuron should be activated or not. This means that it will decide whether the neuron's input to the network is important or not in the process of prediction using simpler mathematical operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79608a27",
   "metadata": {},
   "source": [
    "# Q2. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1c4f6b",
   "metadata": {},
   "source": [
    "1. Linear or Identity Activation Function.\n",
    "2. Non-linear Activation Function.\n",
    "3. Sigmoid or Logistic Activation Function.\n",
    "4. Tanh or hyperbolic tangent Activation Function.\n",
    "5. ReLU (Rectified Linear Unit) Activation Function.\n",
    "6. Leaky ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6142559e",
   "metadata": {},
   "source": [
    "# Q3. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb95f236",
   "metadata": {},
   "source": [
    "Activation functions play a crucial role in neural networks by introducing non-linearity to the model. They determine the output of a neuron based on the weighted sum of inputs. The choice of activation function can significantly impact the training process and the performance of a neural network. Here are some key aspects:\n",
    "\n",
    "Non-Linearity: Activation functions introduce non-linearity to the model, allowing neural networks to learn complex patterns and relationships in the data. Without non-linear activation functions, a neural network would be limited to representing only linear functions.\n",
    "\n",
    "Gradient Flow and Training Speed: Activation functions affect the flow of gradients during backpropagation, which is crucial for updating the network's weights. Activation functions with well-defined gradients and a large range of non-zero derivatives, such as ReLU and its variants, enable more efficient training by preventing the vanishing gradient problem and allowing faster convergence.\n",
    "\n",
    "Output Range: Activation functions determine the range of values that a neuron can output. Some activation functions, like sigmoid and tanh, squash the output between a specific range (e.g., [0, 1] or [-1, 1]). This can be beneficial in certain scenarios where bounded outputs are desired, such as binary classification problems. However, it can also lead to the saturation of neurons and the vanishing gradient problem, particularly in deeper networks.\n",
    "\n",
    "Handling of Zero Inputs: Activation functions should handle zero inputs appropriately. Some activation functions, such as ReLU, map negative inputs to zero, effectively creating sparse representations and facilitating model interpretability. However, this can cause dead neurons when the gradients become zero for negative inputs, leading to a loss of information. Other activation functions, like Leaky ReLU and Parametric ReLU (PReLU), address this issue by allowing a small non-zero output for negative inputs.\n",
    "\n",
    "Smoothness: Activation functions that are smooth (have continuous derivatives) can be advantageous in terms of model robustness, as they exhibit more stable gradients during training. Functions like sigmoid and tanh are smooth and can provide more refined outputs compared to step functions like ReLU, which have discontinuous derivatives.\n",
    "\n",
    "Computational Efficiency: The computational complexity of an activation function can impact the overall efficiency of training and inference. Some activation functions, like ReLU, are computationally efficient to compute, while others, such as exponential functions like softmax, can be more expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc4bd9d",
   "metadata": {},
   "source": [
    "# Q4. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa5144d",
   "metadata": {},
   "source": [
    "The sigmoid activation function, also known as the logistic function, is a widely used activation function in neural networks. It maps the weighted sum of inputs to a value between 0 and 1, which can be interpreted as a probability. The sigmoid function is defined as:\n",
    "\n",
    "f(x) = 1 / (1 + exp(-x))\n",
    "\n",
    "Here are some key aspects of the sigmoid activation function:\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Non-linearity: The sigmoid function introduces non-linearity to the network, allowing it to learn and model complex relationships in the data.\n",
    "\n",
    "Smoothness: The sigmoid function is a smooth and continuous function, which can provide more refined outputs compared to step functions like ReLU.\n",
    "\n",
    "Bounded Output: The sigmoid function maps the output to the range [0, 1], which can be advantageous for tasks like binary classification, where the output represents a probability or a decision boundary.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "Vanishing Gradient: The sigmoid function is prone to the vanishing gradient problem, especially in deeper networks. The gradients of the sigmoid function are relatively small, especially for large input values, which can cause the gradients to diminish rapidly during backpropagation. This can result in slow convergence or difficulty in learning long-term dependencies.\n",
    "\n",
    "Output Saturation: The sigmoid function saturates at the extreme ends of its range, meaning that very large positive or negative inputs are mapped to values close to 1 or 0, respectively. This can cause neurons to become unresponsive, leading to the vanishing gradient problem and difficulty in training deep networks.\n",
    "\n",
    "Biased Outputs: The output of the sigmoid function is biased towards the extremes of the range, resulting in outputs that are close to 0 or 1. This can limit the discriminative power of the network, especially in cases where the data distribution is imbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e742adbf",
   "metadata": {},
   "source": [
    "# Q5. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c204264c",
   "metadata": {},
   "source": [
    "The rectified linear activation function or ReLU for short is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero.\n",
    "\n",
    "Efficiency: ReLu is faster to compute than the sigmoid function, and its derivative is faster to compute. This makes a significant difference to training and inference time for neural networks: only a constant factor, but constants can matter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dfcab5",
   "metadata": {},
   "source": [
    "# Q6. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62829ad",
   "metadata": {},
   "source": [
    "Efficiency: ReLu is faster to compute than the sigmoid function, and its derivative is faster to compute. This makes a significant difference to training and inference time for neural networks: only a constant factor, but constants can matter. Simplicity: ReLu is simple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4771ebf",
   "metadata": {},
   "source": [
    "# Q7. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3336c1ad",
   "metadata": {},
   "source": [
    "Leaky Rectified Linear Unit, or Leaky ReLU, is a type of activation function based on a ReLU, but it has a small slope for negative values instead of a flat slope. The slope coefficient is determined before training, i.e. it is not learnt during training.\n",
    "\n",
    "Because ReLU is known for vanishing gradients, since any values less than zero are mapped to zero. This is true regardless of the number of layers. LeakyReLU on the other hand, maps the values less than zeros to a very small positive number. This prevents vanishing gradient from occurring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c517d76c",
   "metadata": {},
   "source": [
    "# Q8. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeeef82",
   "metadata": {},
   "source": [
    "The softmax function is used as the activation function in the output layer of neural network models that predict a multinomial probability distribution. That is, softmax is used as the activation function for multi-class classification problems where class membership is required on more than two class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9eb3f09",
   "metadata": {},
   "source": [
    "# Q9. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9e33a7",
   "metadata": {},
   "source": [
    "The hyperbolic tangent (tanh) activation function is another commonly used activation function in neural networks. It is similar to the sigmoid function but differs in terms of the range of its output. The tanh function maps the weighted sum of inputs to a value between -1 and 1. Mathematically, it is defined as:\n",
    "\n",
    "f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n",
    "\n",
    "Here are some key aspects of the tanh activation function:\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Non-linearity: Like the sigmoid function, the tanh function introduces non-linearity to the network, enabling it to learn complex relationships in the data.\n",
    "\n",
    "Zero-Centered Output: Unlike the sigmoid function, the tanh function outputs values centered around zero (-1 to 1). This can be advantageous in certain cases as it helps alleviate the bias towards extreme values and can aid in the learning process.\n",
    "\n",
    "Stronger Activation: The tanh function produces stronger activations compared to the sigmoid function, which can be beneficial when dealing with deep networks or complex data distributions.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "Vanishing Gradient: Similar to the sigmoid function, the tanh function is also susceptible to the vanishing gradient problem, particularly in deep networks. The gradients can become small for larger inputs, leading to slow convergence or difficulty in learning long-term dependencies.\n",
    "\n",
    "In comparison to the sigmoid function, the tanh function has a symmetric range that includes negative values. This can be useful in situations where the input or output data has negative values or exhibits symmetry around zero. However, the tanh function shares some of the drawbacks of the sigmoid function, such as the vanishing gradient problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
