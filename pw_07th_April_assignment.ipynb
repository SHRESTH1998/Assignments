{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6c9c355",
   "metadata": {},
   "source": [
    "# Q1.Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9469b2db",
   "metadata": {},
   "source": [
    "Polynomial functions and kernel functions are closely related in machine learning algorithms, particularly in the context of Support Vector Machines (SVM) and kernel methods. Kernel functions are a mathematical concept used to transform data into a higher-dimensional feature space, allowing for more complex decision boundaries. Polynomial functions can be used as specific types of kernel functions to achieve this transformation.\n",
    "\n",
    "Polynomial functions are one type of kernel function that can be used to create nonlinear decision boundaries. The polynomial kernel function calculates the similarity or dot product between pairs of data points in the transformed space, which corresponds to a polynomial function of the original feature space.\n",
    "\n",
    "The polynomial kernel function is defined as:\n",
    "\n",
    "K(x, y) = (gamma * (x^T * y) + coef0)^degree\n",
    "\n",
    "where:\n",
    "\n",
    "1. x and y are input feature vectors,\n",
    "2. gamma is a hyperparameter that controls the influence of individual training instances,\n",
    "3. coef0 is a constant term,\n",
    "4. degree is the degree of the polynomial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd90e9e1",
   "metadata": {},
   "source": [
    "# Q2. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f8ac45",
   "metadata": {},
   "source": [
    "To implement an SVM with a polynomial kernel in Python using Scikit-learn, you can follow these steps:\n",
    "\n",
    "1. Import the necessary libraries:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "2. Prepare your dataset by splitting it into training and testing sets, and optionally scaling the features:\n",
    "python\n",
    "Copy code\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features (optional but recommended)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "3. Create an instance of the SVC class with the kernel parameter set to 'poly' for polynomial kernel:\n",
    "python\n",
    "Copy code\n",
    "# Create an SVM classifier with a polynomial kernel\n",
    "svm = SVC(kernel='poly')\n",
    "\n",
    "4. Fit the SVM classifier to the training data:\n",
    "python\n",
    "Copy code\n",
    "# Fit the SVM classifier to the training data\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "5. Make predictions on the test data:\n",
    "python\n",
    "Copy code\n",
    "# Make predictions on the test data\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "6. Evaluate the performance of the SVM model using appropriate metrics:\n",
    "python\n",
    "Copy code\n",
    "# Evaluate the performance of the SVM model\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "Remember to replace X with your input feature matrix and y with your target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89c90f5",
   "metadata": {},
   "source": [
    "# Q3. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd800f1b",
   "metadata": {},
   "source": [
    "The value of ε can affect the number of support vectors used to construct the regression function. The bigger ε, the fewer support vectors are selected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b9a122",
   "metadata": {},
   "source": [
    "# Q4. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a55ca9",
   "metadata": {},
   "source": [
    "1. Kernel Function:\n",
    "The kernel function in SVR determines the type of nonlinearity that can be captured by the model. Common kernel functions include linear, polynomial, and radial basis function (RBF). The choice of kernel function depends on the complexity of the underlying relationship between the features and the target variable. Here are some examples:\n",
    "\n",
    "A. Linear Kernel: Use a linear kernel when the relationship between the features and target variable is expected to be linear. For example, if you are predicting housing prices based on features like area and number of bedrooms, a linear kernel may be suitable.\n",
    "\n",
    "B. Polynomial Kernel: Use a polynomial kernel when the relationship is expected to be polynomial. For instance, if the relationship between the features and target variable follows a quadratic or cubic pattern, a polynomial kernel can capture these higher-degree interactions.\n",
    "\n",
    "C. RBF Kernel: Use an RBF kernel when the relationship is highly nonlinear or if you are unsure about the specific shape of the relationship. The RBF kernel can capture complex patterns and is a good default choice.\n",
    "\n",
    "2. C Parameter:\n",
    "The C parameter in SVR controls the trade-off between the model's complexity and the amount of deviation allowed in the training error. It determines the penalty for errors made by the model. Consider the following examples:\n",
    "\n",
    "A. Increasing C: If you want to reduce the training error to a greater extent and are less concerned about overfitting, you can increase the C parameter. This encourages the model to fit the training data more closely and results in a narrower margin.\n",
    "\n",
    "B. Decreasing C: If you want to prioritize a wider margin and are willing to tolerate more errors, you can decrease the C parameter. This allows for a more flexible margin and promotes better generalization to unseen data.\n",
    "\n",
    "3. Epsilon Parameter:\n",
    "The epsilon parameter (ε) in SVR defines the width of the tube around the regression line within which errors are considered negligible. It controls the margin of tolerance for errors. Consider the following examples:\n",
    "\n",
    "A. Increasing epsilon: If you want to allow larger errors in your predictions, you can increase the epsilon parameter. This results in a wider tube, making the model more robust to noise or outliers.\n",
    "\n",
    "B. Decreasing epsilon: If you want to enforce stricter error control and prioritize accurate predictions, you can decrease the epsilon parameter. This leads to a narrower tube, making the model more sensitive to errors and potentially improving accuracy.\n",
    "\n",
    "4. Gamma Parameter:\n",
    "The gamma parameter in SVR defines the reach of influence of individual training samples. It controls the shape and flexibility of the decision boundary. Here are some examples:\n",
    "\n",
    "A. Increasing gamma: If the underlying relationship between the features and target variable is complex or if there are fine-grained distinctions in the target variable, you may want to increase the gamma parameter. This allows the model to focus on a smaller neighborhood of training samples, resulting in a more intricate decision boundary.\n",
    "\n",
    "B. Decreasing gamma: If you want a smoother decision boundary or if there is high noise or outliers in the dataset, you can decrease the gamma parameter. This allows the model to consider a wider neighborhood of training samples and leads to a smoother decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa7fe10",
   "metadata": {},
   "source": [
    "# Q5. Ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00860c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "Best Parameters: {'C': 10, 'gamma': 0.1, 'kernel': 'linear'}\n",
      "Best Score: 0.9583333333333334\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 54>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Train the tuned classifier on the entire dataset\u001b[39;00m\n\u001b[0;32m     53\u001b[0m svm_classifier_tuned \u001b[38;5;241m=\u001b[39m SVC(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbest_params)\n\u001b[1;32m---> 54\u001b[0m svm_classifier_tuned\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_scaled\u001b[49m, y)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Save the trained classifier to a file\u001b[39;00m\n\u001b[0;32m     57\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(svm_classifier_tuned, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msvm_classifier.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data (scaling)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of the SVC classifier\n",
    "svm_classifier = SVC()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svm_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict the labels for the testing data\n",
    "y_pred = svm_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the performance of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': [0.1, 0.01, 0.001],\n",
    "    'kernel': ['rbf', 'linear', 'poly']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=svm_classifier, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters and best score from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "svm_classifier_tuned = SVC(**best_params)\n",
    "svm_classifier_tuned.fit(X_scaled, y)\n",
    "\n",
    "# Save the trained classifier to a file\n",
    "joblib.dump(svm_classifier_tuned, \"svm_classifier.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd557fb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
