{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8738107a",
   "metadata": {},
   "source": [
    "# Q1. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d07050",
   "metadata": {},
   "source": [
    "Anomaly detection identifies suspicious activity that falls outside of your established normal patterns of behavior. A solution protects your system in real-time from instances that could result in significant financial losses, data breaches, and other harmful events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8787991",
   "metadata": {},
   "source": [
    "# Q2. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627fd269",
   "metadata": {},
   "source": [
    "An anomaly is defined as an observation that does not conform to the expected normal behaviour. To detect anomalies, modelling and encapsulating normal data is still an open problem, especially if only normal (non-anomalous) data is available for training time, making it a challenging problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42185038",
   "metadata": {},
   "source": [
    "# Q3. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea6509c",
   "metadata": {},
   "source": [
    "The main difference between supervised and unsupervised anomaly detection is the approach involved, where supervised approach makes use of predefined algorithms and AI training, while unsupervised approach uses a general outlier-detection mechanism based on pattern matching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b5a597",
   "metadata": {},
   "source": [
    "# Q4. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cac780c",
   "metadata": {},
   "source": [
    "There are three main classes of anomaly detection techniques: unsupervised, semi-supervised, and supervised."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb3f657",
   "metadata": {},
   "source": [
    "# Q5. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbbae30",
   "metadata": {},
   "source": [
    "Distance-based anomaly detection methods make certain assumptions about the data in order to identify anomalies. Here are the main assumptions made by distance-based anomaly detection methods:\n",
    "\n",
    "Distance Metric: Distance-based anomaly detection methods assume that the data can be represented in a feature space where the concept of distance between data points is meaningful. These methods rely on distance metrics such as Euclidean distance or Mahalanobis distance to measure the dissimilarity between data points.\n",
    "\n",
    "Normal Behavior Distribution: Distance-based methods assume that the majority of the data points follow a normal or expected behavior pattern. Anomalies are considered as data points that deviate significantly from this normal behavior. The assumption is that normal data points are densely packed together, while anomalies are located far away from the normal cluster.\n",
    "\n",
    "Density-Based Outlier Detection: Some distance-based methods, such as Local Outlier Factor (LOF), assume that anomalies are characterized by their lower density compared to the surrounding normal data points. Anomalies are expected to have fewer neighboring data points within a certain distance threshold, indicating their isolation from the rest of the data.\n",
    "\n",
    "Independent and Identically Distributed (IID) Data: Distance-based methods often assume that the data points are independent and identically distributed, meaning that each data point is sampled from the same underlying distribution and does not depend on previous or future data points. This assumption allows the methods to compute distances between data points without considering temporal or sequential dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8f4866",
   "metadata": {},
   "source": [
    "# Q6. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e282c12e",
   "metadata": {},
   "source": [
    "The Local Outlier Factor (LOF) algorithm is an unsupervised anomaly detection method which computes the local density deviation of a given data point with respect to its neighbors. It considers as outliers the samples that have a substantially lower density than their neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc852ef4",
   "metadata": {},
   "source": [
    "# Q7. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dfd918",
   "metadata": {},
   "source": [
    "The key parameters of the Isolation Forest algorithm are:\n",
    "\n",
    "n_estimators: It determines the number of isolation trees to be created. Increasing the number of trees improves the performance of the algorithm but also increases the computational cost. It is a hyperparameter that needs to be tuned.\n",
    "\n",
    "max_samples: It specifies the number of samples to be drawn from the dataset to create each isolation tree. A smaller value can lead to more randomness and increase the diversity of trees but might also result in less accurate results. The default value is \"auto,\" which selects a maximum of 256 samples.\n",
    "\n",
    "contamination: It represents the expected proportion of anomalies in the dataset. It is used to define the threshold for classifying data points as anomalies. The default value is \"auto,\" which estimates the contamination based on the dataset's size and assumes a contamination rate of 0.1%.\n",
    "\n",
    "max_features: It determines the number of features to consider when splitting a node in the isolation tree. The algorithm randomly selects a subset of features for each split. A lower value can increase the randomness and diversity of trees but might also result in less accurate results. The default value is 1.0, which considers all features.\n",
    "\n",
    "bootstrap: It determines whether to use bootstrapping for sampling the data points when creating each isolation tree. Bootstrapping introduces additional randomness into the algorithm. The default value is False."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7ee8fa",
   "metadata": {},
   "source": [
    "# Q8. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f12a128",
   "metadata": {},
   "source": [
    "To calculate the anomaly score using KNN with K=10, we need to determine the distance from the data point to its 10th nearest neighbor. If the data point has only 2 neighbors of the same class within a radius of 0.5, it means that the 10th nearest neighbor will be at a distance greater than 0.5. In this case, the anomaly score will be relatively high because the data point is far away from its 10th nearest neighbor compared to the majority of the points in its class.\n",
    "\n",
    "However, it's important to note that the anomaly score in KNN depends on the distances to the K nearest neighbors, not just the 10th nearest neighbor. Therefore, to get a more accurate anomaly score, we need to consider the distances to all the K nearest neighbors and take into account the distances of those neighbors to their own K nearest neighbors as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4505a7",
   "metadata": {},
   "source": [
    "# Q9. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8b620a",
   "metadata": {},
   "source": [
    "In the Isolation Forest algorithm, the anomaly score for a data point is calculated based on its average path length compared to the average path length of the trees in the forest. The anomaly score is inversely proportional to the average path length.\n",
    "\n",
    "Given that the average path length of the trees is not provided, we cannot determine the exact anomaly score for a data point with an average path length of 5.0 compared to the average path length of the trees. The anomaly score is computed by comparing the data point's average path length to the average path length of a normal point in a random dataset.\n",
    "\n",
    "To calculate the anomaly score, we need to know the average path length of the trees in the Isolation Forest. Without that information, we cannot provide a specific anomaly score for the given scenario."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
