{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40d9ba02",
   "metadata": {},
   "source": [
    "# Q1. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caa83da",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors Algorithm. The k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a0dc5c",
   "metadata": {},
   "source": [
    "# Q2. ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cd2671",
   "metadata": {},
   "source": [
    "The choice of k will largely depend on the input data as data with more outliers or noise will likely perform better with higher values of k. Overall, it is recommended to have an odd number for k to avoid ties in classification, and cross-validation tactics can help you choose the optimal k for your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945f5b78",
   "metadata": {},
   "source": [
    "# Q3.Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb240982",
   "metadata": {},
   "source": [
    "The main difference between Regression and Classification algorithms that Regression algorithms are used to predict the continuous values such as price, salary, age, etc. and Classification algorithms are used to predict/Classify the discrete values such as Male or Female, True or False, Spam or Not Spam, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb8faa4",
   "metadata": {},
   "source": [
    "# Q4. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8b35e2",
   "metadata": {},
   "source": [
    "The k-nearest neighbour classification (k-NN) is one of the most popular distance-based algorithms. This classification is based on measuring the distances between the test sample and the training samples to determine the final classification output. The traditional k-NN classifier works naturally with numerical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04898c6c",
   "metadata": {},
   "source": [
    "# Q5. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28769a88",
   "metadata": {},
   "source": [
    "The “Curse of Dimensionality” is a tongue in cheek way of stating that there's a ton of space in high-dimensional data sets. The size of the data space grows exponentially with the number of dimensions. This means that the size of your data set must also grow exponentially in order to keep the same density."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb05a91",
   "metadata": {},
   "source": [
    "# Q6. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f209640",
   "metadata": {},
   "source": [
    "The idea in kNN methods is to identify 'k' samples in the dataset that are similar or close in the space. Then we use these 'k' samples to estimate the value of the missing data points. Each sample's missing values are imputed using the mean value of the 'k'-neighbors found in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a342d6",
   "metadata": {},
   "source": [
    "# Q7. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2afe52",
   "metadata": {},
   "source": [
    "KNN Classifier:\n",
    "\n",
    "1. Classification task: In KNN classification, the algorithm predicts the class label of a new data point based on the majority vote of its K nearest neighbors in the feature space. The predicted class is determined by the most frequently occurring class among the K nearest neighbors.\n",
    "\n",
    "2. Performance: KNN classifier works well when the decision boundary between classes is nonlinear or when the classes have complex relationships. However, it can struggle with high-dimensional data and imbalanced class distributions. Additionally, the performance of KNN classifier heavily depends on the choice of K and the distance metric used.\n",
    "\n",
    "3. Use case: KNN classifier is commonly used in problems such as image recognition, text categorization, and sentiment analysis, where the task is to classify data points into predefined categories.\n",
    "\n",
    "KNN Regressor:\n",
    "\n",
    "1. Regression task: In KNN regression, the algorithm predicts the target value of a new data point by averaging the target values of its K nearest neighbors. The predicted value is determined by the mean or median of the target values of the K nearest neighbors.\n",
    "\n",
    "2. Performance: KNN regressor is effective when there are clear trends and patterns in the data. It performs well in situations where the relationship between the features and the target variable is nonlinear. However, similar to the KNN classifier, it can be sensitive to the choice of K and the distance metric. It may also struggle with high-dimensional data and outliers.\n",
    "\n",
    "3. Use case: KNN regressor is commonly used in problems such as housing price prediction, stock market analysis, and demand forecasting, where the task is to predict continuous target variables.\n",
    "\n",
    "In summary, the choice between the KNN classifier and regressor depends on the problem at hand:\n",
    "\n",
    "1. Use KNN classifier for classification tasks when the goal is to assign data points to predefined classes and when the decision boundary is expected to be nonlinear or complex.\n",
    "\n",
    "2. Use KNN regressor for regression tasks when the goal is to predict continuous target variables and when there are clear patterns and trends in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ac20cf",
   "metadata": {},
   "source": [
    "# Q8.Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9128ee",
   "metadata": {},
   "source": [
    "The KNN uses neighborhood classification as the predication value of the new query. It has advantages - nonparametric architecture, simple and powerful, requires no traning time, but it also has disadvantage - memory intensive, classification and estimation are slow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce759fa4",
   "metadata": {},
   "source": [
    "# Q9. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44df0d8",
   "metadata": {},
   "source": [
    "Euclidean distance is the shortest path between source and destination which is a straight line but Manhattan distance is sum of all the real distances between source(s) and destination(d) and each distance are always the straight lines "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ab4d22",
   "metadata": {},
   "source": [
    "# Q10. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642d2a39",
   "metadata": {},
   "source": [
    "Feature scaling is essential for machine learning algorithms that calculate distances between data. If not scaled, the feature with a higher value range starts dominating when calculating distances. KNN which uses Euclidean distance is one such algorithm which essentially require scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe73b48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
