{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "080460a7-6d16-4a83-ba13-dfe5893184fe",
      "metadata": {
        "id": "080460a7-6d16-4a83-ba13-dfe5893184fe"
      },
      "source": [
        "Q1. Ans"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3443f2bc-b618-4eb9-94b9-9f75281da8ad",
      "metadata": {
        "id": "3443f2bc-b618-4eb9-94b9-9f75281da8ad"
      },
      "source": [
        "R-Squared (R² or the coefficient of determination) is a statistical measure in a regression model that determines the proportion of variance in the dependent variable that can be explained by the independent variable. In other words, r-squared shows how well the data fit the regression model (the goodness of fit).\n",
        "\n",
        "R 2 = 1 − sum squared regression (SSR) total sum of squares (SST) , = 1 − ∑ ( y i − y i ^ ) 2 ∑ ( y i − y ¯ ) 2 . The sum squared regression is the sum of the residuals squared, and the total sum of squares is the sum of the distance the data is away from the mean all squared."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2711395c-6fbc-49f3-930a-2cefb3e50cb8",
      "metadata": {
        "id": "2711395c-6fbc-49f3-930a-2cefb3e50cb8"
      },
      "source": [
        "Q2. Ans"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6db30387-08cc-43b7-8c25-3b3be9157b88",
      "metadata": {
        "id": "6db30387-08cc-43b7-8c25-3b3be9157b88"
      },
      "source": [
        "Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases when the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected.\n",
        "\n",
        "In general, as sample size increases, the difference between expected adjusted r-squared and expected r-squared approaches zero; in theory this is because expected r-squared becomes less biased. the standard error of adjusted r-squared would get smaller approaching zero in the limit."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdbc0745-015e-416a-8776-da63555eb880",
      "metadata": {
        "id": "bdbc0745-015e-416a-8776-da63555eb880"
      },
      "source": [
        "Q3. Ans"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "009c52ee-cb42-4365-a056-8957012ca0ac",
      "metadata": {
        "id": "009c52ee-cb42-4365-a056-8957012ca0ac"
      },
      "source": [
        "Clearly, it is better to use Adjusted R-squared when there are multiple variables in the regression model. This would allow us to compare models with differing numbers of independent variables."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fe955a3-f2e4-4f5e-92f0-a1f9e1a03bf8",
      "metadata": {
        "id": "0fe955a3-f2e4-4f5e-92f0-a1f9e1a03bf8"
      },
      "source": [
        "Q4. Ans"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aecb3d6d-d9fb-434d-87aa-56d17dd55ca0",
      "metadata": {
        "id": "aecb3d6d-d9fb-434d-87aa-56d17dd55ca0"
      },
      "source": [
        "The Mean absolute error (MAE) represents the average of the absolute difference between the actual and predicted values in the dataset. It measures the average of the residuals in the dataset.\n",
        "The Mean absolute error is calculated by adding up all the absolute errors and dividing them by the number of errors.\n",
        "\n",
        "Mean Squared Error (MSE) represents the average of the squared difference between the original and predicted values in the data set. It measures the variance of the residuals.\n",
        "\n",
        "MSE = Σ(ŷi – yi)2 / n\n",
        "where:\n",
        "\n",
        "Σ is a symbol that means “sum”\n",
        "ŷi is the predicted value for the ith observation\n",
        "yi is the observed value for the ith observation\n",
        "n is the sample size\n",
        "\n",
        "Root Mean Squared Error (RMSE) is the square root of Mean Squared error. It measures the standard deviation of residuals.\n",
        "It is calculated as:\n",
        "\n",
        "RMSE = √Σ(ŷi – yi)2 / n\n",
        "\n",
        "where:\n",
        "\n",
        "Σ is a symbol that means “sum”\n",
        "ŷi is the predicted value for the ith observation\n",
        "yi is the observed value for the ith observation\n",
        "n is the sample size"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4fef5f1-5340-4882-8342-8c9e46ed0532",
      "metadata": {
        "id": "a4fef5f1-5340-4882-8342-8c9e46ed0532"
      },
      "source": [
        "Q5. Ans"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99151754-d9e7-4b78-8e5d-d4517e4c4df5",
      "metadata": {
        "id": "99151754-d9e7-4b78-8e5d-d4517e4c4df5"
      },
      "source": [
        "MSE\n",
        "Advantage: The MSE is great for ensuring that our trained model has no outlier predictions with huge errors, since the MSE puts larger weight on theses errors due to the squaring part of the function.\n",
        "\n",
        "Disadvantage: If our model makes a single very bad prediction, the squaring part of the function magnifies the error.\n",
        "\n",
        "RMSE\n",
        "Advantages: RMSE is better in terms of reflecting performance when dealing with large error values. RMSE is more useful when lower residual values are preferred.\n",
        "\n",
        "Disadvantages: One major drawback of RMSE is its sensitivity to outliers and the outliers have to be removed for it to function properly. RMSE increases with an increase in the size of the test sample. This is an issue when we calculate the results on different test samples.\n",
        "\n",
        "MAE\n",
        "Advantages: It is calculated by taking the absolute difference between the predicted values and the actual values and averaging it across the dataset. Mathematically speaking, it is the arithmetic average of absolute errors.\n",
        "\n",
        "Disadvantaegs: i) Beacuse of its non-differentiable nature of graphs, will need various optimizers like gradient descent to make them differentiable.\n",
        "ii) Bigger error terms are failed to be punished. Mean of the squared distances between actual and predicted values."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "121cec5f-54d4-4a1e-87cf-0d6e5cb7bc4d",
      "metadata": {
        "id": "121cec5f-54d4-4a1e-87cf-0d6e5cb7bc4d"
      },
      "source": [
        "Q6. Ans"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5afa8a9-7319-4535-9850-bb5fcc020c90",
      "metadata": {
        "id": "b5afa8a9-7319-4535-9850-bb5fcc020c90"
      },
      "source": [
        "Lasso regression is a regularization technique. It is used over regression methods for a more accurate prediction. This model uses shrinkage. Shrinkage is where data values are shrunk towards a central point as the mean. The lasso procedure encourages simple, sparse models (i.e. models with fewer parameters).\n",
        "\n",
        "Similar to the lasso regression, ridge regression puts a similar constraint on the coefficients by introducing a penalty factor. However, while lasso regression takes the magnitude of the coefficients, ridge regression takes the square. Ridge regression is also referred to as L2 Regularization.\n",
        "\n",
        "Lasso tends to do well if there are a small number of significant parameters and the others are close to zero (ergo: when only a few predictors actually influence the response). Ridge works well if there are many large parameters of about the same value (ergo: when most predictors impact the response)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea21ae3b-86bc-45bf-868e-f0f557414fa0",
      "metadata": {
        "id": "ea21ae3b-86bc-45bf-868e-f0f557414fa0"
      },
      "source": [
        "Q7. Ans"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c00a426-51ea-4751-9c41-dcd3bf1b7eb5",
      "metadata": {
        "id": "3c00a426-51ea-4751-9c41-dcd3bf1b7eb5"
      },
      "source": [
        "Dropout is a regularization technique that prevents neural networks from overfitting. Regularization methods like L1 and L2 reduce overfitting by modifying the cost function. Dropout on the other hand, modify the network itself. It randomly drops neurons from the neural network during training in each iteration.\n",
        "\n",
        "Regularization is a technique that penalizes the coefficient. In an overfit model, the coefficients are generally inflated. Thus, Regularization adds penalties to the parameters and avoids them weigh heavily. The coefficients are added to the cost function of the linear equation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34132999-a88f-4aac-adce-16777a5d1f9f",
      "metadata": {
        "id": "34132999-a88f-4aac-adce-16777a5d1f9f"
      },
      "source": [
        "Q8.  Ans"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d66cd577-a2a3-42d7-8573-b0b8da03a76b",
      "metadata": {
        "id": "d66cd577-a2a3-42d7-8573-b0b8da03a76b"
      },
      "source": [
        "Regularization leads to dimensionality reduction, which means the machine learning model is built using a lower dimensional dataset. This generally leads to a high bias errror. If regularization is performed before training the model, a perfect balance between bias-variance tradeoff must be used."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. Ans"
      ],
      "metadata": {
        "id": "PR3OYiScW3oS"
      },
      "id": "PR3OYiScW3oS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this scenario, we have Model A with an RMSE (Root Mean Squared Error) of 10 and Model B with an MAE (Mean Absolute Error) of 8.\n",
        "\n",
        "To determine which model is the better performer, we need to consider the specific context and requirements of the problem.\n",
        "\n",
        "If we prioritize the minimization of large errors, the RMSE is more appropriate as it penalizes larger errors more heavily due to the squaring operation. In this case, Model A with an RMSE of 10 would indicate that, on average, the predictions deviate from the actual values by 10 units.\n",
        "\n",
        "On the other hand, if we prioritize the overall magnitude of errors without emphasizing the distinction between small and large errors, the MAE is more suitable. In this case, Model B with an MAE of 8 would suggest that, on average, the predictions deviate from the actual values by 8 units.\n",
        "\n",
        "Ultimately, the choice between the two models depends on the specific requirements and goals of the project. If the emphasis is on reducing larger errors, Model A with a lower RMSE may be preferred. If the focus is on overall accuracy, Model B with a lower MAE may be favored."
      ],
      "metadata": {
        "id": "IbwQpxwXW7aK"
      },
      "id": "IbwQpxwXW7aK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3601de68-97df-44d5-9860-2a9cfc830197",
      "metadata": {
        "id": "3601de68-97df-44d5-9860-2a9cfc830197"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10. Ans"
      ],
      "metadata": {
        "id": "nqXbDmyoW9FH"
      },
      "id": "nqXbDmyoW9FH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this scenario, we have Model A using Ridge regularization with a regularization parameter of 0.1 and Model B using Lasso regularization with a regularization parameter of 0.5.\n",
        "\n",
        "To determine which model is the better performer, we need to consider the specific characteristics of the problem and the goals of the analysis.\n",
        "\n",
        "Ridge regularization (L2 regularization) adds a penalty term to the cost function that is proportional to the square of the magnitude of the coefficients. It helps to prevent overfitting by shrinking the coefficients towards zero while still keeping all features in the model. The regularization parameter controls the strength of regularization, where smaller values indicate less regularization.\n",
        "\n",
        "Lasso regularization (L1 regularization) also adds a penalty term to the cost function, but this penalty is proportional to the absolute value of the coefficients. Lasso regularization has the property of feature selection, as it can drive the coefficients of irrelevant features to exactly zero, effectively removing them from the model. Similarly, the regularization parameter controls the strength of regularization, where smaller values indicate less regularization.\n",
        "\n",
        "To determine which model is better, we need to assess their performance on a specific task, such as prediction accuracy or generalization to unseen data. It is recommended to evaluate the models using appropriate evaluation metrics, such as mean squared error (MSE) or R-squared, on a validation or test dataset. By comparing the performance metrics, we can determine which model performs better on the specific task at hand.\n",
        "\n",
        "When choosing between Ridge and Lasso regularization, there are some trade-offs and limitations to consider:\n",
        "\n",
        "Ridge regularization tends to be more effective when the dataset contains a large number of features, and there is a possibility that all or most of them are relevant for the task. It helps to reduce the impact of multicollinearity by distributing the weight among correlated features. Ridge regularization may not perform well in scenarios where feature selection is critical.\n",
        "\n",
        "Lasso regularization is effective when the dataset has many features, and we suspect that only a subset of them is truly relevant. Lasso can perform feature selection by driving the coefficients of irrelevant features to zero. However, Lasso regularization may struggle when dealing with multicollinearity, as it tends to arbitrarily select one of the correlated features.\n",
        "\n",
        "The choice of regularization parameter (lambda or alpha) is important and depends on the specific dataset and problem. It needs to be tuned carefully to strike a balance between regularization strength and model performance."
      ],
      "metadata": {
        "id": "KG1yY5ASXPS-"
      },
      "id": "KG1yY5ASXPS-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e754a053-d700-4358-b136-f9d1706a2ab2",
      "metadata": {
        "id": "e754a053-d700-4358-b136-f9d1706a2ab2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}