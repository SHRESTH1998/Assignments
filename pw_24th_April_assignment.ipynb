{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28ff54fa",
   "metadata": {},
   "source": [
    "# Q1. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4457f8",
   "metadata": {},
   "source": [
    "PCA can be defined as the orthogonal projection of the data onto a lower dimensional linear space, known as the principal subspace, such that the variance of the projected data is maximized.\n",
    "\n",
    "In predictive modelling PCA is particular useful as a data pre-processing technique. PCA serves as a tool for exploratory data analysis and outlier detection, but as well for dimensionality reduction when the number of variables outnumbers the sample size (d>n)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78bfffc",
   "metadata": {},
   "source": [
    "# Q2. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d811dd21",
   "metadata": {},
   "source": [
    "PCA seeks to solve a sequence of optimization problems. The first in the sequence is the unconstrained problem maximizeuTSuuTu,u∈Rp. Since uTu=‖u‖22=‖u‖‖u‖, the above unconstrained problem is equivalent to the constrained problem maximizeuTSusubject touTu=1.\n",
    "\n",
    "PCA can be used to reduce the dimensionality of the data by creating a set of derived variables that are linear combinations of the original variables. The values of the derived variables are given in the columns of the scores matrix Z."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc29e81",
   "metadata": {},
   "source": [
    "# Q3. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dd4e3a",
   "metadata": {},
   "source": [
    "Covariance-based PCA is equivalent to MLPCA whenever the variance-covariance matrix of the measurement errors is assumed diagonal with equal elements on its diagonal. The measurement error variance parameter can then be estimated by applying the probabilistic principal component analysis (PPCA) model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ec32cb",
   "metadata": {},
   "source": [
    "# Q4. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b1e905",
   "metadata": {},
   "source": [
    "The choice of the number of principal components in PCA (Principal Component Analysis) can have a significant impact on the performance of the technique. Here's how the choice of the number of principal components can affect the performance:\n",
    "\n",
    "1. Information Retention: Each principal component captures a certain amount of variance in the data. By selecting a smaller number of principal components, you retain only a portion of the total variance in the original data. Therefore, the more principal components you include, the more information you retain from the original dataset. However, including too many principal components may result in overfitting and the inclusion of noise in the data.\n",
    "\n",
    "2. Dimensionality Reduction: PCA is often used as a dimensionality reduction technique, where the goal is to reduce the number of dimensions in the dataset while retaining most of the relevant information. By selecting a smaller number of principal components, you effectively reduce the dimensionality of the data. This can be beneficial for reducing computational complexity, improving interpretability, and mitigating the curse of dimensionality.\n",
    "\n",
    "3. Model Performance: The number of principal components can impact the performance of subsequent models or algorithms that use the reduced dataset. Including more principal components may lead to a more accurate representation of the original data, but it can also increase model complexity and introduce noise. On the other hand, selecting too few principal components may result in an under-representation of the data, leading to poorer model performance. It's essential to strike a balance between the number of principal components and the performance of the downstream tasks.\n",
    "\n",
    "4. Computational Efficiency: The computational cost of performing PCA increases with the number of principal components. Selecting a smaller number of principal components can reduce the computational time required for performing PCA, making it more efficient for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f558da2",
   "metadata": {},
   "source": [
    "# Q5. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a6e78b",
   "metadata": {},
   "source": [
    "The basic idea when using PCA as a tool for feature selection is to select variables according to the magnitude (from largest to smallest in absolute values) of their coefficients (loadings).\n",
    "\n",
    "Principal component analysis (PCA) is a powerful technique for reducing the dimensionality of data and extracting meaningful features. It can help you simplify complex data sets, visualize patterns, and improve the performance of machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95404e79",
   "metadata": {},
   "source": [
    "# Q6.Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa0a6cc",
   "metadata": {},
   "source": [
    "Fore Data Science\n",
    "\n",
    "Principal component analysis, or PCA, is a statistical procedure that allows you to summarize the information content in large data tables by means of a smaller set of “summary indices” that can be more easily visualized and analyzed.\n",
    "\n",
    "For Machine Learning\n",
    "\n",
    "1. PCA is used to visualize multidimensional data.\n",
    "2. It is used to reduce the number of dimensions in healthcare data.\n",
    "3. PCA can help resize an image.\n",
    "4. It can be used in finance to analyze stock data and forecast returns.\n",
    "5. PCA helps to find patterns in the high-dimensional datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820bf5c5",
   "metadata": {},
   "source": [
    "# Q7.Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51829d88",
   "metadata": {},
   "source": [
    "Variance: Variance is the spread of the data in a dataset. In PCA, the variables are transformed in such a way that they explain variance of the dataset in decreasing manner. Co-variance: Covariance provides a measure of the strength of the correlation between two or more sets of random variates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5b8092",
   "metadata": {},
   "source": [
    "# Q8. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cde2a43",
   "metadata": {},
   "source": [
    "PCA works by finding the directions of maximum variance in the data set and projecting the data onto these directions. The principal components are ordered by the amount of variance they explain and are used for feature selection, data compression, clustering, and classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa562153",
   "metadata": {},
   "source": [
    "# Q9. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71406dbb",
   "metadata": {},
   "source": [
    "PCA generally tries to find the lower-dimensional surface to project the high-dimensional data. PCA works by considering the variance of each attribute because the high attribute shows the good split between the classes, and hence it reduces the dimensionality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
