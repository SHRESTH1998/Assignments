{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47e78f38",
   "metadata": {},
   "source": [
    "# Q1. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63409231",
   "metadata": {},
   "source": [
    "Gradient boosting Regression calculates the difference between the current prediction and the known correct target value. This difference is called residual. After that Gradient boosting Regression trains a weak model that maps features to that residual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c2f063",
   "metadata": {},
   "source": [
    "# Q2. Ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "619b6296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 36.00000000564406\n",
      "R-squared: -3.5000000007055077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.models = []\n",
    "        self.weights = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Initialize the predicted values to the mean of the target variable\n",
    "        predictions = np.full(len(X), np.mean(y))\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            # Compute the residuals as the differences between the true values and the predicted values\n",
    "            residuals = y - predictions\n",
    "            \n",
    "            # Fit a regression tree to the residuals\n",
    "            model = DecisionTreeRegressor()\n",
    "            model.fit(X, residuals)\n",
    "            \n",
    "            # Update the predictions by adding the predictions of the new model multiplied by the learning rate\n",
    "            predictions += self.learning_rate * model.predict(X)\n",
    "            \n",
    "            # Save the model and its weight (learning rate)\n",
    "            self.models.append(model)\n",
    "            self.weights.append(self.learning_rate)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Make predictions by summing the predictions of all models multiplied by their weights\n",
    "        predictions = np.zeros(len(X))\n",
    "        \n",
    "        for model, weight in zip(self.models, self.weights):\n",
    "            predictions += weight * model.predict(X)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "class DecisionTreeRegressor:\n",
    "    def fit(self, X, y):\n",
    "        self.feature_index, self.threshold, self.value = self._find_best_split(X, y)\n",
    "        if self.feature_index is None:\n",
    "            self.value = np.mean(y)\n",
    "            return\n",
    "        \n",
    "        left_indices = X[:, self.feature_index] < self.threshold\n",
    "        right_indices = ~left_indices\n",
    "        \n",
    "        self.left = DecisionTreeRegressor()\n",
    "        self.left.fit(X[left_indices], y[left_indices])\n",
    "        \n",
    "        self.right = DecisionTreeRegressor()\n",
    "        self.right.fit(X[right_indices], y[right_indices])\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.feature_index is None:\n",
    "            return self.value\n",
    "        \n",
    "        predictions = np.zeros(len(X))\n",
    "        left_indices = X[:, self.feature_index] < self.threshold\n",
    "        right_indices = ~left_indices\n",
    "        \n",
    "        predictions[left_indices] = self.left.predict(X[left_indices])\n",
    "        predictions[right_indices] = self.right.predict(X[right_indices])\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def _find_best_split(self, X, y):\n",
    "        best_loss = np.inf\n",
    "        best_feature_index = None\n",
    "        best_threshold = None\n",
    "        \n",
    "        for feature_index in range(X.shape[1]):\n",
    "            feature_values = X[:, feature_index]\n",
    "            unique_values = np.unique(feature_values)\n",
    "            \n",
    "            for threshold in unique_values:\n",
    "                left_indices = feature_values < threshold\n",
    "                right_indices = ~left_indices\n",
    "                \n",
    "                left_loss = np.mean((y[left_indices] - np.mean(y[left_indices])) ** 2)\n",
    "                right_loss = np.mean((y[right_indices] - np.mean(y[right_indices])) ** 2)\n",
    "                total_loss = left_loss + right_loss\n",
    "                \n",
    "                if total_loss < best_loss:\n",
    "                    best_loss = total_loss\n",
    "                    best_feature_index = feature_index\n",
    "                    best_threshold = threshold\n",
    "        \n",
    "        return best_feature_index, best_threshold, None\n",
    "\n",
    "# Example usage\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([2, 4, 6, 8, 10])\n",
    "\n",
    "model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "X_test = np.array([[6], [7], [8]])\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "mse = np.mean((y - model.predict(X)) ** 2)\n",
    "ssr = np.sum((y - model.predict(X)) ** 2)\n",
    "sst = np.sum((y - np.mean(y)) ** 2)\n",
    "r_squared = 1 - ssr / sst\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r_squared)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b2291f",
   "metadata": {},
   "source": [
    "# Q3. Ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "532db320",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot clone object '<__main__.GradientBoostingRegressor object at 0x0000029BB4D1E4C0>' (type <class '__main__.GradientBoostingRegressor'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' method.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[0;32m     12\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mGradientBoostingRegressor(),\n\u001b[0;32m     13\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mparam_grid,\n\u001b[0;32m     14\u001b[0m     scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Use negative mean squared error as the evaluation metric\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m  \u001b[38;5;66;03m# Perform 5-fold cross-validation\u001b[39;00m\n\u001b[0;32m     16\u001b[0m )\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Fit the grid search to the data\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Print the best hyperparameters and the corresponding MSE\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Hyperparameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:789\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    786\u001b[0m cv_orig \u001b[38;5;241m=\u001b[39m check_cv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv, y, classifier\u001b[38;5;241m=\u001b[39mis_classifier(estimator))\n\u001b[0;32m    787\u001b[0m n_splits \u001b[38;5;241m=\u001b[39m cv_orig\u001b[38;5;241m.\u001b[39mget_n_splits(X, y, groups)\n\u001b[1;32m--> 789\u001b[0m base_estimator \u001b[38;5;241m=\u001b[39m \u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    791\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs, pre_dispatch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_dispatch)\n\u001b[0;32m    793\u001b[0m fit_and_score_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m    794\u001b[0m     scorer\u001b[38;5;241m=\u001b[39mscorers,\n\u001b[0;32m    795\u001b[0m     fit_params\u001b[38;5;241m=\u001b[39mfit_params,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    801\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    802\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:79\u001b[0m, in \u001b[0;36mclone\u001b[1;34m(estimator, safe)\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m     74\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot clone object. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     75\u001b[0m                 \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou should provide an instance of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     76\u001b[0m                 \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscikit-learn estimator instead of a class.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     77\u001b[0m             )\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 79\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m     80\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot clone object \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     81\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit does not seem to be a scikit-learn \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     82\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator as it does not implement a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     83\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mget_params\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m method.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mrepr\u001b[39m(estimator), \u001b[38;5;28mtype\u001b[39m(estimator))\n\u001b[0;32m     84\u001b[0m             )\n\u001b[0;32m     86\u001b[0m klass \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\n\u001b[0;32m     87\u001b[0m new_object_params \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mget_params(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot clone object '<__main__.GradientBoostingRegressor object at 0x0000029BB4D1E4C0>' (type <class '__main__.GradientBoostingRegressor'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' method."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.05, 0.01],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# Create the grid search object\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=GradientBoostingRegressor(),\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',  # Use negative mean squared error as the evaluation metric\n",
    "    cv=5  # Perform 5-fold cross-validation\n",
    ")\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding MSE\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best MSE:\", -grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3b665c",
   "metadata": {},
   "source": [
    "# Q4. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a829d4",
   "metadata": {},
   "source": [
    "Decision trees are used as the weak learner in gradient boosting. Specifically regression trees are used that output real values for splits and whose output can be added together, allowing subsequent models outputs to be added and “correct” the residuals in the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648bb15a",
   "metadata": {},
   "source": [
    "# Q5. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60412c28",
   "metadata": {},
   "source": [
    "In gradient boosting, we predict and adjust our predictions in the opposite (negative gradient) direction. This achieves the opposite (minimize the loss). Since, the loss of a model inversely relates to its performance and accuracy, doing so improves its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dc17fa",
   "metadata": {},
   "source": [
    "# Q6. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b14a3eb",
   "metadata": {},
   "source": [
    "Sequential Ensemble Learning\n",
    "It is a boosting technique where the outputs from individual weak learners associate sequentially during the training phase. The performance of the model is boosted by assigning higher weights to the samples that are incorrectly classified.\n",
    "\n",
    "1. Initialize the ensemble: The algorithm starts by initializing the ensemble with a simple model, typically a decision tree with a small depth, called the base learner.\n",
    "\n",
    "2. Fit the base learner: The base learner is fitted to the training data, and its predictions are computed.\n",
    "\n",
    "3. Compute the residual errors: The difference between the actual target values and the predictions of the base learner is calculated. These differences are referred to as residual errors.\n",
    "\n",
    "4. Fit the next weak learner: A new weak learner is fitted to the residual errors. The objective of this weak learner is to learn the patterns in the residual errors that were not captured by the previous base learner.\n",
    "\n",
    "5. Update the ensemble: The predictions of the new weak learner are added to the predictions of the previous base learner. This update is done by multiplying the predictions of the weak learner by a small learning rate, which controls the contribution of each weak learner to the ensemble.\n",
    "\n",
    "6. Repeat steps 3 to 5: The process is repeated by computing the residual errors based on the updated ensemble's predictions and fitting a new weak learner to the residual errors. This process continues for a specified number of iterations or until a stopping criterion is met.\n",
    "\n",
    "7. Final ensemble prediction: The final prediction of the gradient boosting ensemble is obtained by summing the predictions of all the weak learners, weighted by their learning rates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944a9d9c",
   "metadata": {},
   "source": [
    "# Q7. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d2147e",
   "metadata": {},
   "source": [
    "Constructing the mathematical intuition of the Gradient Boosting algorithm involves understanding the key components and steps involved. Here are the main steps:\n",
    "\n",
    "1. Define the loss function: The first step is to define a loss function that measures the error between the predicted values and the actual target values. Common loss functions for regression problems include mean squared error (MSE) and mean absolute error (MAE).\n",
    "\n",
    "2. Initialize the model: The algorithm starts by initializing the model with a simple estimator, typically a decision tree with a shallow depth or a constant value. This initial model serves as the starting point for the ensemble.\n",
    "\n",
    "3. Compute the negative gradient: The negative gradient of the loss function with respect to the predictions of the current model is computed. The negative gradient indicates the direction of steepest descent in the loss landscape, allowing the algorithm to identify the areas where the current model performs poorly.\n",
    "\n",
    "4. Fit a weak learner: A weak learner, such as a decision tree, is fitted to the negative gradient. The weak learner is trained to approximate the negative gradient, aiming to correct the errors made by the current model.\n",
    "\n",
    "5. Update the model: The predictions of the weak learner are multiplied by a small learning rate, referred to as the shrinkage parameter. This scaling factor controls the contribution of each weak learner to the ensemble. The predictions of the weak learner are added to the predictions of the current model, updating the model's predictions.\n",
    "\n",
    "6. Repeat steps 3 to 5: Steps 3 to 5 are repeated iteratively, with each iteration focusing on fitting a new weak learner to the negative gradient of the loss function. The algorithm continues to add weak learners to the ensemble, gradually reducing the loss and improving the overall prediction accuracy.\n",
    "\n",
    "7. Final ensemble prediction: The final prediction of the Gradient Boosting ensemble is obtained by summing the predictions of all the weak learners, weighted by their learning rates.\n",
    "\n",
    "The mathematical intuition of Gradient Boosting lies in the iterative optimization process, where each weak learner is trained to correct the mistakes made by the previous learners. By minimizing the loss function in the direction of steepest descent, the algorithm converges to an ensemble of weak learners that collectively provide a strong predictive model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
