{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bf50e814-5d11-46b4-a26a-539761752317",
      "metadata": {
        "id": "bf50e814-5d11-46b4-a26a-539761752317"
      },
      "source": [
        "Q1. Ans"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eec40350-1eb1-4e49-aa4b-700f439ebbb7",
      "metadata": {
        "id": "eec40350-1eb1-4e49-aa4b-700f439ebbb7"
      },
      "source": [
        "It is a univariate feature selection technique where the predictive power of each variable is evaluated and the relationship between input and output variable of interest is considered for this method.\n",
        "\n",
        "Filter methods measure the relevance of features by their correlation with dependent variable"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afb48ef9-7e37-4dbe-8262-a4ea1f9f75cf",
      "metadata": {
        "id": "afb48ef9-7e37-4dbe-8262-a4ea1f9f75cf"
      },
      "source": [
        "Q2. Ans"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1de1e714-6882-424a-bb80-a6be099bb805",
      "metadata": {
        "id": "1de1e714-6882-424a-bb80-a6be099bb805"
      },
      "source": [
        "Filter methods perform the feature selection independently of construction of the classification model. Wrapper methods iteratively select or eliminate a set of features using the prediction accuracy of the classification model. In embedded methods the feature selection is an integral part of the classification model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbec0bba-8fe4-4456-9391-444805d7acb0",
      "metadata": {
        "id": "fbec0bba-8fe4-4456-9391-444805d7acb0"
      },
      "source": [
        "Q3. Ans"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49742bd3-6ecc-4b46-a8de-3df432a21e90",
      "metadata": {
        "id": "49742bd3-6ecc-4b46-a8de-3df432a21e90"
      },
      "source": [
        "Embedded Methods\n",
        "It's implemented by algorithms that have their own built-in feature selection methods. Some of the most popular examples of these methods are LASSO and RIDGE regression which have inbuilt penalization functions to reduce overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8aa35f3d-1d85-4a03-b3c8-e67b1c55e3cc",
      "metadata": {
        "id": "8aa35f3d-1d85-4a03-b3c8-e67b1c55e3cc"
      },
      "source": [
        "Q4. Ans"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa070028-d861-4338-ac7c-42ea06fc3db0",
      "metadata": {
        "id": "fa070028-d861-4338-ac7c-42ea06fc3db0"
      },
      "source": [
        "The common disadvantage of filter methods is that they ignore the interaction with the classifier and each feature is considered independently thus ignoring feature dependencies In addition, it is not clear how to determine the threshold point for rankings to select only the required features and exclude noise."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a559f22-1d89-4ad1-8f5c-b76584404acf",
      "metadata": {
        "id": "4a559f22-1d89-4ad1-8f5c-b76584404acf"
      },
      "source": [
        "Q5. Ans"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "565dbf63-f75a-4e83-87e4-253a4481a343",
      "metadata": {
        "id": "565dbf63-f75a-4e83-87e4-253a4481a343"
      },
      "source": [
        "For large data you should use the Filter approaches because these approaches are rapid and for small size of data it is better to use Wrapper (KNN, SVM,...) approaches because they are slower than the Filter approaches. or you can combine the two approaches to have better results than the two approaches."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93ef12e6-fdeb-4ebb-bc61-be29af09f59b",
      "metadata": {
        "id": "93ef12e6-fdeb-4ebb-bc61-be29af09f59b"
      },
      "source": [
        "Q6. Ans"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "762a0a85-90a8-497d-80f0-f82d79ebf767",
      "metadata": {
        "id": "762a0a85-90a8-497d-80f0-f82d79ebf767"
      },
      "source": [
        "To choose the most pertinent attributes for a predictive model using the Filter Method, you can follow these steps:\n",
        "\n",
        "Understand the Problem: Start by gaining a clear understanding of the problem you are trying to solve and the goals of the predictive model. Identify the key factors or attributes that are most likely to influence customer churn in the context of the telecom industry.\n",
        "\n",
        "Data Exploration: Perform exploratory data analysis (EDA) on the dataset to gain insights into the available attributes. Look for patterns, correlations, and relationships between the attributes and the target variable (customer churn). This step helps you understand the data distribution, identify missing values, handle outliers, and visualize relationships between variables.\n",
        "\n",
        "Define Evaluation Metric: Determine the evaluation metric that will be used to assess the performance of the predictive model. For customer churn, common evaluation metrics include accuracy, precision, recall, F1 score, or area under the receiver operating characteristic (ROC) curve.\n",
        "\n",
        "Feature Selection Methods: Choose appropriate feature selection methods for your dataset. In this case, you are considering the Filter Method. The Filter Method uses statistical measures to assess the relationship between each attribute and the target variable, independent of any specific machine learning algorithm.\n",
        "\n",
        "Select Relevant Measures: Select appropriate statistical measures to evaluate the relevance of each attribute. Common measures include correlation coefficient, mutual information, chi-square test, ANOVA F-test, etc. The choice of measures depends on the type of data (numeric or categorical) and the target variable.\n",
        "\n",
        "Rank the Attributes: Calculate the relevant measure for each attribute and rank them based on their relevance to the target variable. You can use statistical libraries like SciPy or scikit-learn in Python to calculate these measures.\n",
        "\n",
        "Set a Threshold: Set a threshold or select a fixed number of attributes that you want to include in your predictive model. You can use domain expertise, previous research, or the specific requirements of the problem to guide your selection.\n",
        "\n",
        "Select the Pertinent Attributes: Choose the top-ranked attributes based on the threshold or fixed number selected in the previous step. These attributes are considered the most pertinent for the predictive model.\n",
        "\n",
        "Build and Evaluate the Model: Use the selected attributes to build a predictive model, such as a logistic regression, decision tree, or random forest classifier. Train the model on a training dataset, tune hyperparameters if necessary, and evaluate its performance on a separate validation or test dataset using the chosen evaluation metric.\n",
        "\n",
        "Iterate and Refine: If the model's performance is not satisfactory, you can iterate and refine the attribute selection process by experimenting with different statistical measures, thresholds, or feature selection methods."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a91b24b9-f0e5-497a-811f-78e46fb110af",
      "metadata": {
        "id": "a91b24b9-f0e5-497a-811f-78e46fb110af"
      },
      "source": [
        "Q7. Ans"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To select the most relevant features for predicting the outcome of a soccer match using the Embedded method, you can follow these steps:\n",
        "\n",
        "Understand the Problem: Gain a clear understanding of the problem you are trying to solve and the goals of the predictive model. Identify the key factors that are most likely to influence the outcome of a soccer match. These factors can include player statistics, team rankings, historical performance, playing style, and other relevant information.\n",
        "\n",
        "Data Preprocessing: Preprocess the dataset by handling missing values, encoding categorical variables, and scaling numerical features. Ensure that the data is in a suitable format for the subsequent steps.\n",
        "\n",
        "Feature Engineering: Create new features or derive meaningful features from the existing dataset that might provide additional insights and predictive power. This step can involve aggregating player statistics, calculating team performance indicators, or incorporating external data sources.\n",
        "\n",
        "Define Evaluation Metric: Determine the evaluation metric that will be used to assess the performance of the predictive model for soccer match outcome prediction. Common evaluation metrics include accuracy, precision, recall, F1 score, or area under the receiver operating characteristic (ROC) curve.\n",
        "\n",
        "Select a Machine Learning Algorithm: Choose a suitable machine learning algorithm for predicting the outcome of soccer matches. Common algorithms include logistic regression, random forest, gradient boosting, or neural networks. The choice of algorithm depends on the specific requirements of the problem and the available computational resources.\n",
        "\n",
        "Embedded Feature Selection: Use an embedded feature selection technique that incorporates feature selection within the model training process. These techniques assess the importance of features while fitting the model. For example:\n",
        "\n",
        "L1 Regularization (Lasso): L1 regularization penalizes the coefficients of less important features, effectively shrinking them to zero. Features with non-zero coefficients are considered more important and selected for the model.\n",
        "\n",
        "Tree-based Methods: Tree-based algorithms, such as random forest or gradient boosting, inherently provide feature importances. Features with higher importances are considered more relevant and selected for the model.\n",
        "\n",
        "Recursive Feature Elimination (RFE): RFE is an iterative feature selection technique that starts with all features and removes the least important features based on model performance at each iteration. This process continues until the desired number of features is reached.\n",
        "\n",
        "Model Training and Evaluation: Train the machine learning model using the selected features. Split the dataset into training and test sets, and evaluate the model's performance on the test set using the chosen evaluation metric. Adjust hyperparameters and model settings as needed to optimize performance.\n",
        "\n",
        "Iterative Refinement: If the model's performance is not satisfactory, you can iterate and refine the feature selection process by experimenting with different embedded techniques, adjusting regularization parameters, or engineering new features."
      ],
      "metadata": {
        "id": "djAcvRh_hON9"
      },
      "id": "djAcvRh_hON9"
    },
    {
      "cell_type": "markdown",
      "id": "88d92bb5-6818-4952-a58b-732a587687b8",
      "metadata": {
        "id": "88d92bb5-6818-4952-a58b-732a587687b8"
      },
      "source": [
        "Q8. Ans"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62e38d23-7021-4780-b414-2645beac7e41",
      "metadata": {
        "id": "62e38d23-7021-4780-b414-2645beac7e41"
      },
      "source": [
        "To select the best set of features for predicting the price of a house using the Wrapper method, you can follow these steps:\n",
        "\n",
        "Understand the Problem: Gain a clear understanding of the problem you are trying to solve and the goals of the predictive model. Identify the key factors or features that are most likely to influence the house price, such as size, location, age, number of rooms, amenities, etc.\n",
        "\n",
        "Data Preprocessing: Preprocess the dataset by handling missing values, encoding categorical variables, and scaling numerical features. Ensure that the data is in a suitable format for the subsequent steps.\n",
        "\n",
        "Define Evaluation Metric: Determine the evaluation metric that will be used to assess the performance of the predictive model for house price prediction. Common evaluation metrics include mean squared error (MSE), mean absolute error (MAE), or R-squared.\n",
        "\n",
        "Select a Machine Learning Algorithm: Choose a suitable machine learning algorithm for predicting the house prices. Common algorithms include linear regression, decision trees, random forest, gradient boosting, or neural networks. The choice of algorithm depends on the specific requirements of the problem, the available computational resources, and the assumptions about the data.\n",
        "\n",
        "Wrapper Feature Selection: Use a wrapper feature selection technique that involves training and evaluating the model using different subsets of features. This technique assesses the performance of the model with each feature subset and selects the best set of features based on model performance. For example:\n",
        "\n",
        "Forward Selection: Start with an empty set of features and iteratively add one feature at a time, evaluating the model's performance at each step. Select the feature that provides the best improvement in model performance until a stopping criterion is met.\n",
        "\n",
        "Backward Elimination: Start with all features and iteratively remove one feature at a time, evaluating the model's performance after removing each feature. Remove the feature that has the least impact on model performance until a stopping criterion is met.\n",
        "\n",
        "Recursive Feature Elimination (RFE): RFE is an iterative feature selection technique that starts with all features and removes the least important features based on model performance at each iteration. This process continues until the desired number of features is reached.\n",
        "\n",
        "Model Training and Evaluation: Train the machine learning model using the selected features. Split the dataset into training and test sets, and evaluate the model's performance on the test set using the chosen evaluation metric. Adjust hyperparameters and model settings as needed to optimize performance.\n",
        "\n",
        "Iterate and Refine: If the model's performance is not satisfactory, you can iterate and refine the feature selection process by experimenting with different wrapper techniques, adjusting stopping criteria, or engineering new features."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-iieT2xEhd8l"
      },
      "id": "-iieT2xEhd8l",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}