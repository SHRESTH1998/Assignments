{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5d0f082",
   "metadata": {},
   "source": [
    "# Q1. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e76d39d",
   "metadata": {},
   "source": [
    "GridSearchCV tries all the combinations of the values passed in the dictionary and evaluates the model for each combination using the Cross-Validation method. Hence after using this function we get accuracy/loss for every combination of hyperparameters and we can choose the one with the best performance.\n",
    "\n",
    "Grid search is a process that searches exhaustively through a manually specified subset of the hyperparameter space of the targeted algorithm. Random search, on the other hand, selects a value for each hyperparameter independently using a probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c8eb83",
   "metadata": {},
   "source": [
    "\n",
    "# Q2. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d42ec5",
   "metadata": {},
   "source": [
    "In Grid Search, we try every combination of a preset list of values of the hyper-parameters and choose the best combination based on the cross-validation score.\n",
    "\n",
    "Random search tries random combinations of a range of values (we have to define the number iterations). It is good at testing a wide range of values and normally it reaches a very good combination very fast, but the problem that it doesn’t guarantee to give the best parameter combination.\n",
    "\n",
    "Random search is faster than grid search and should always be used when you have a large parameter space.\n",
    "\n",
    "The grid method is best used in large crime scenes such as fields or woods. Several searchers, or a line of them, move alongside each other from one end of the area to be searched to the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d725d15d",
   "metadata": {},
   "source": [
    "# Q3. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f635fde",
   "metadata": {},
   "source": [
    "In simple words, data leakage can be defined as a scenario when ML model already has information of test data in training data, but this information would not be available at the time of prediction, called data leakage. It causes high performance while training set, but perform poorly in deployment or production.\n",
    "\n",
    "Data leakage happens when a model is trained using information about the target variable that will not be available when the model is released into production.\n",
    "\n",
    "Data exposed in transit — Data transmitted via emails, API calls, chat rooms, and other communications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a2692d",
   "metadata": {},
   "source": [
    "# Q4. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9e16bf",
   "metadata": {},
   "source": [
    "One of the best ways to get rid of data leakage is to perform k-fold cross validation where the overall data is divided into k parts. After dividing into k parts, we use each part as the cross-validation data and the remaining as training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dd44f4",
   "metadata": {},
   "source": [
    "# Q5. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602d35ef",
   "metadata": {},
   "source": [
    "A confusion matrix is a table that allows you to visualize the performance of a classification model. You can also use the information in it to calculate measures that can help you determine the usefulness of the model.\n",
    "\n",
    "A confusion matrix represents the prediction summary in matrix form. It shows how many prediction are correct and incorrect per class. It helps in understanding the classes that are being confused by model as other class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4669ddcd",
   "metadata": {},
   "source": [
    "# Q6. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d73067",
   "metadata": {},
   "source": [
    "Precision is a metric that measures the accuracy of positive predictions. It is the number of true positive predictions divided by the number of true positive predictions plus false positive predictions.\n",
    "\n",
    "Recall tells us about how well the model identifies true positives. Out of all the patients who have the disease, how many were correctly identified? In this case, the number of true positives is 0. The number of false negatives is 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755ba154",
   "metadata": {},
   "source": [
    "# Q7. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a111bbc",
   "metadata": {},
   "source": [
    "A confusion matrix is a table that allows you to visualize the performance of a classification model. You can also use the information in it to calculate measures that can help you determine the usefulness of the model. Rows represent predicted classifications, while columns represent the true classes from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eb12ff",
   "metadata": {},
   "source": [
    "# Q8. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b92c96",
   "metadata": {},
   "source": [
    "Confusion matrices can be used to calculate performance metrics for classification models. Of the many performance metrics used, the most common are accuracy, precision, recall, and F1 score.\n",
    "\n",
    "Accuracy:\n",
    "The formula for calculating accuracy, based on the chart above, is \n",
    "\n",
    "(TP+TN)/(TP+FP+FN+TN) \n",
    "\n",
    "or all true positive and true negative cases divided by the number of all cases.\n",
    "\n",
    "Precision:\n",
    "Precision is the measure of true positives over the number of total positives predicted by your model. The formula for precision can be written as: \n",
    "\n",
    "TP/(TP+FP). \n",
    "\n",
    "Recall:\n",
    "Recall (a.k.a sensitivity) is the measure of your true positive over the count of actual positive outcomes. The formula for recall can be expressed as: \n",
    "\n",
    "TP/(TP+FN).\n",
    "\n",
    "F1 Score:\n",
    "The F1 score is the harmonic mean between precision and recall. The formula for the F1 score can be expressed as: \n",
    "\n",
    "2(p*r)/(p+r) \n",
    "\n",
    "where ‘p’ is precision and ‘r’ is recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37db158",
   "metadata": {},
   "source": [
    "# Q9. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5fbad4",
   "metadata": {},
   "source": [
    "The values in the confusion matrix, along with accuracy, provide a comprehensive understanding of a model's performance by considering true positives, true negatives, false positives, and false negatives. These values allow for the calculation of additional metrics such as precision, recall, and F1-score, which provide a more nuanced evaluation of the model's predictive capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f628c798",
   "metadata": {},
   "source": [
    "# Q10. Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d13070",
   "metadata": {},
   "source": [
    "1. Analyze class imbalances.\n",
    "2. Evaluate accuracy and error rates.\n",
    "3. Assess precision and recall.\n",
    "4. Investigate false positives and false negatives.\n",
    "5. Consider external factors.\n",
    "6. Mitigate biases and limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a432c9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
